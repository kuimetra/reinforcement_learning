{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28e96d65110588d",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.001941Z",
     "start_time": "2024-05-09T16:47:25.366298Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.088228Z",
     "start_time": "2024-05-09T16:47:26.002885Z"
    }
   },
   "id": "16bd03c7f0454be5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.092286Z",
     "start_time": "2024-05-09T16:47:26.090216Z"
    }
   },
   "id": "cfcd967e8ee8617a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "num_iterations = 500\n",
    "horizon = 2048\n",
    "learning_rate = 3e-4\n",
    "num_epochs = 10\n",
    "minibatch_size = 10\n",
    "gamma = 0.99\n",
    "gae = 0.95"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.095926Z",
     "start_time": "2024-05-09T16:47:26.092671Z"
    }
   },
   "id": "80f7b9c17bd4a10"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.network(state)\n",
    "        log_std = self.log_std.expand_as(mean)\n",
    "        std = torch.exp(log_std)\n",
    "        return Normal(mean, std)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.099718Z",
     "start_time": "2024-05-09T16:47:26.097183Z"
    }
   },
   "id": "35daddcba65777a8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def generate_episode(env, actor, T):\n",
    "    states, actions, rewards, masks, log_probs = [], [], [], [], []\n",
    "    state, _ = env.reset()\n",
    "    for t in range(T):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        dist = actor(state_tensor)\n",
    "        action = dist.sample().squeeze(0)\n",
    "        log_prob = dist.log_prob(action).squeeze()\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.numpy())\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        masks.append(1 - done)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return torch.FloatTensor(np.vstack(states)), torch.FloatTensor(\n",
    "        np.vstack(actions)), rewards, masks, torch.FloatTensor(log_probs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.103546Z",
     "start_time": "2024-05-09T16:47:26.100522Z"
    }
   },
   "id": "18e7b8a5e18797fc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def compute_rewards_to_go(rewards, masks, gamma=0.99):\n",
    "    rtg = []\n",
    "    discounted_sum = 0\n",
    "    for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "        discounted_sum = reward + gamma * discounted_sum * mask\n",
    "        rtg.insert(0, discounted_sum)\n",
    "    return rtg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.104132Z",
     "start_time": "2024-05-09T16:47:26.103183Z"
    }
   },
   "id": "ba69e81406aff3b3"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, values, masks, gamma=0.99, lam=0.95):\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32)\n",
    "    gae = 0.0\n",
    "    for t in reversed(range(T - 1)):\n",
    "        td_error = rewards[t] + gamma * masks[t] * values[t + 1] - values[t]\n",
    "        gae = td_error + gamma * lam * masks[t] * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.107978Z",
     "start_time": "2024-05-09T16:47:26.105892Z"
    }
   },
   "id": "f073cb8fecb2c186"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def update_policy(actor, critic, states, actions, log_probs_old, advantages, rewards_to_go, minibatch_size,\n",
    "                  clip_param=0.2):\n",
    "    total_samples = states.size(0)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        perm = torch.randperm(total_samples)\n",
    "\n",
    "        states_perm, actions_perm, log_probs_old_perm, advantages_perm, rewards_to_go_perm = \\\n",
    "            states[perm], actions[perm], log_probs_old[perm], advantages[perm], rewards_to_go[perm]\n",
    "\n",
    "        # Process each minibatch\n",
    "        for i in range(0, total_samples, minibatch_size):\n",
    "            states_batch = states_perm[i:i + minibatch_size]\n",
    "            actions_batch = actions_perm[i:i + minibatch_size]\n",
    "            log_probs_old_batch = log_probs_old_perm[i:i + minibatch_size]\n",
    "            advantages_batch = advantages_perm[i:i + minibatch_size]\n",
    "            rewards_to_go_batch = rewards_to_go_perm[i:i + minibatch_size]\n",
    "\n",
    "            # Recompute distribution for current policy\n",
    "            dist = actor(states_batch)\n",
    "            log_probs = dist.log_prob(actions_batch).squeeze()\n",
    "\n",
    "            # Calculate the ratio of new to old probabilities\n",
    "            ratios = torch.exp(log_probs - log_probs_old_batch.detach())\n",
    "\n",
    "            # Clipping the ratio to stabilize training\n",
    "            clipped_ratios = torch.clamp(ratios, 1 - clip_param, 1 + clip_param)\n",
    "\n",
    "            # Calculate the clipped and unclipped objective\n",
    "            unclipped_objective = ratios * advantages_batch\n",
    "            clipped_objective = clipped_ratios * advantages_batch\n",
    "\n",
    "            # Negative sign to run gradient ascent\n",
    "            actor_loss = -torch.min(unclipped_objective, clipped_objective).mean()\n",
    "\n",
    "            values = critic(states_batch).squeeze()\n",
    "\n",
    "            # Critic loss\n",
    "            critic_loss = nn.MSELoss()(values, rewards_to_go_batch)\n",
    "\n",
    "            # Optimize the actor\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # Optimize the critic\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.114518Z",
     "start_time": "2024-05-09T16:47:26.110195Z"
    }
   },
   "id": "bc4cb84877bb0044"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.441029Z",
     "start_time": "2024-05-09T16:47:26.113426Z"
    }
   },
   "id": "68dcc3d908f6a6e2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266: UserWarning: Error detected in AddmmBackward0. No forward pass information available. Enable detect anomaly during forward pass for more information. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:96.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 12\u001B[0m\n\u001B[1;32m      9\u001B[0m advantages \u001B[38;5;241m=\u001B[39m compute_advantages(rewards, values, masks)\n\u001B[1;32m     10\u001B[0m advantages \u001B[38;5;241m=\u001B[39m (advantages \u001B[38;5;241m-\u001B[39m advantages\u001B[38;5;241m.\u001B[39mmean()) \u001B[38;5;241m/\u001B[39m (advantages\u001B[38;5;241m.\u001B[39mstd() \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1e-10\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[43mupdate_policy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcritic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_probs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madvantages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrewards_to_go\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mminibatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 44\u001B[0m, in \u001B[0;36mupdate_policy\u001B[0;34m(actor, critic, states, actions, log_probs_old, advantages, rewards_to_go, minibatch_size, clip_param)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Optimize the actor\u001B[39;00m\n\u001B[1;32m     43\u001B[0m actor_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 44\u001B[0m \u001B[43mactor_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m actor_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;66;03m# Optimize the critic\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "for iteration in range(num_iterations):\n",
    "    states, actions, rewards, masks, log_probs = generate_episode(env, actor, horizon)\n",
    "    \n",
    "    rewards_to_go = compute_rewards_to_go(rewards, masks)\n",
    "    rewards_to_go = torch.tensor(rewards_to_go)\n",
    "\n",
    "    values = critic(states).squeeze()\n",
    "\n",
    "    advantages = compute_advantages(rewards, values, masks)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "    update_policy(actor, critic, states, actions, log_probs, advantages, rewards_to_go, minibatch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-09T16:47:26.613990Z",
     "start_time": "2024-05-09T16:47:26.442248Z"
    }
   },
   "id": "52c0344325b8be4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-09T16:47:26.616522Z"
    }
   },
   "id": "7a5a5d8f06042ada"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
