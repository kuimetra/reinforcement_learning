{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28e96d65110588d",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:14:37.878649Z",
     "start_time": "2024-05-08T11:14:36.911915Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:14:37.996479Z",
     "start_time": "2024-05-08T11:14:37.879481Z"
    }
   },
   "id": "16bd03c7f0454be5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:14:37.999648Z",
     "start_time": "2024-05-08T11:14:37.997353Z"
    }
   },
   "id": "cfcd967e8ee8617a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 8\tReward: 1788"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 114\u001B[0m\n\u001B[1;32m    111\u001B[0m episode_rewards \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_iterations):\n\u001B[0;32m--> 114\u001B[0m     states, actions, log_probs, rewards, masks \u001B[38;5;241m=\u001B[39m \u001B[43mcollect_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhorizon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m     episode_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(rewards)\n\u001B[1;32m    116\u001B[0m     episode_rewards\u001B[38;5;241m.\u001B[39mappend(episode_reward)\n",
      "Cell \u001B[0;32mIn[4], line 42\u001B[0m, in \u001B[0;36mcollect_data\u001B[0;34m(env, actor, max_steps)\u001B[0m\n\u001B[1;32m     40\u001B[0m dist \u001B[38;5;241m=\u001B[39m actor(state_tensor)\n\u001B[1;32m     41\u001B[0m action \u001B[38;5;241m=\u001B[39m dist\u001B[38;5;241m.\u001B[39msample()\n\u001B[0;32m---> 42\u001B[0m log_prob \u001B[38;5;241m=\u001B[39m \u001B[43mdist\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_prob\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m next_state, reward, terminated, truncated, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m     45\u001B[0m states\u001B[38;5;241m.\u001B[39mappend(state_tensor)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/distributions/normal.py:83\u001B[0m, in \u001B[0;36mNormal.log_prob\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# compute the variance\u001B[39;00m\n\u001B[1;32m     81\u001B[0m var \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     82\u001B[0m log_scale \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m---> 83\u001B[0m     math\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscale, Real) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m )\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;241m-\u001B[39m((value \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloc) \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m var)\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;241m-\u001B[39m log_scale\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;241m-\u001B[39m math\u001B[38;5;241m.\u001B[39mlog(math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m math\u001B[38;5;241m.\u001B[39mpi))\n\u001B[1;32m     89\u001B[0m )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, state):\n",
    "        mean = self.network(state)\n",
    "        std_dev = torch.exp(self.log_std)\n",
    "        return Normal(mean, std_dev)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "def collect_data(env, actor, max_steps):\n",
    "    states, actions, rewards, masks, log_probs = [], [], [], [], []\n",
    "    step_count = 0\n",
    "    state, _ = env.reset()\n",
    "    while step_count < max_steps:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        dist = actor(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action.numpy())\n",
    "\n",
    "        states.append(state_tensor)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        done = terminated or truncated\n",
    "        masks.append(1 - done)\n",
    "        log_probs.append(log_prob)\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "\n",
    "    return states, actions, log_probs, rewards, masks\n",
    "\n",
    "\n",
    "def compute_rewards_to_go(rewards, masks, gamma=0.99):\n",
    "    rtg = []\n",
    "    discounted_sum = 0\n",
    "    for reward, mask in zip(reversed(rewards), reversed(masks)):\n",
    "        discounted_sum = reward + gamma * discounted_sum * mask\n",
    "        rtg.insert(0, discounted_sum)\n",
    "    return rtg\n",
    "\n",
    "\n",
    "def compute_advantages(rewards, values, masks, gamma=0.99, lam=0.95):\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32)\n",
    "    gae = 0.0\n",
    "    for t in reversed(range(T - 1)):\n",
    "        td_error = rewards[t] + gamma * masks[t] * values[t + 1] - values[t]\n",
    "        gae = td_error + gamma * lam * masks[t] * gae\n",
    "        advantages[t] = gae\n",
    "    return advantages\n",
    "\n",
    "\n",
    "def update_policy(actor, critic, optimizer_actor, optimizer_critic, states, actions, log_probs_old, returns, advantages,\n",
    "                  clip_param=0.2):\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.stack(actions)\n",
    "    log_probs_old = torch.stack(log_probs_old)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    dists = actor(states)\n",
    "    values = critic(states).squeeze(-1)\n",
    "    log_probs = dists.log_prob(actions)\n",
    "    ratios = torch.exp(log_probs - log_probs_old)\n",
    "\n",
    "    surr1 = ratios * advantages\n",
    "    surr2 = torch.clamp(ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "    critic_loss = (returns - values).pow(2).mean()\n",
    "\n",
    "    optimizer_actor.zero_grad()\n",
    "    optimizer_critic.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    critic_loss.backward()\n",
    "    optimizer_actor.step()\n",
    "    optimizer_critic.step()\n",
    "\n",
    "\n",
    "actor = Actor(state_dim, action_dim)\n",
    "critic = Critic(state_dim)\n",
    "optimizer_actor = optim.Adam(actor.parameters(), lr=3e-4)\n",
    "optimizer_critic = optim.Adam(critic.parameters(), lr=3e-4)\n",
    "\n",
    "num_iterations = 500\n",
    "horizon = 2048\n",
    "episode_rewards = []\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    states, actions, log_probs, rewards, masks = collect_data(env, actor, horizon)\n",
    "    episode_reward = sum(rewards)\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    returns = compute_rewards_to_go(rewards, masks)\n",
    "    values = critic(torch.stack(states)).detach().squeeze(-1)\n",
    "    advantages = compute_advantages(rewards, values, masks)\n",
    "    update_policy(actor, critic, optimizer_actor, optimizer_critic, states, actions, log_probs, returns, advantages)\n",
    "\n",
    "    print(f\"\\rEpisode: {i}\\tReward: {episode_reward}\", end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-08T11:14:42.950512Z",
     "start_time": "2024-05-08T11:14:38.006793Z"
    }
   },
   "id": "e22d5c10b93537a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(episode_rewards)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-08T11:14:42.948834Z"
    }
   },
   "id": "ad7f033312e91522"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
